# Terminologies of AI, ML and DL

**Artificial Intelligence (AI):**

1. **Classical AI**: Classical AI refers to the traditional approach to artificial intelligence that focuses on building systems based on predefined rules and logic.
2. **Machine learning**: It is the proccess of teaching computer to learn from data.
3. **Deep learning**: DL is a subfield of ML that involves the development of artificial neural networks that are capable of learning from large amounts of data.
4. **Natural language processing (NLP)**: NLP is a subfield of AI that focuses on enabling computers to understand, interpret, and generate human language.
5. **Computer vision**: It focuses on teaching computers to interpret and understand images and videos similar to how humans do.
6. **Robotics**
7. **Expert systems**: Decision maker on a specific field.
8. **Speech recognition**

<br><br>

**Machine Learning (ML):**

1. **Supervised Learning**: It is a type of ML where the algorithm learns from labeled training data, which means that the date has a known output or outcome.
2. **Unsupervised Learning**: It is a type of ML where the algorithm learns from unlabeled training data, which means that the date does not has a known output or outcome.
3. **Semi-Supervised Learning**: ML task where the algorithm learns from a combination of labeled and unlabeled data, take the benefits of both supervised and unsupervised learning.
4. **Reinforcement Learning**: RL is a type of machine learning where an agent learns to take actions in an environment and improve its performance based on rewards and punishments.
5. **Classification**: In classificatio the algorithm predicts a categorical output. 
    - ML task where the algorithm learns to classify or categorize data into predefined classes or categories based on input features.
6. **Regression**: In regression the algorithm predicts a continuous output.
    - ML task where the algorithm learns to predict continuous numerical values or quantities based on input features.
7. **Linear regression** assumes a linear relationship between the independent variables and the dependent variable.
8. **Clustering**: Unsupervised learning task where the algorithm groups similar data points together based on their inherent similarities or patterns.
9. **Feature Extraction**: The process of selecting or extracting relevant features from raw data that are most informative for a given ML task.
10. **Feature Selection**: The process of choosing a subset of relevant features from a larger set of features to improve the performance or efficiency of a ML model.
11. **Overfitting**: A phenomenon where a ML model becomes too specific to the training data, resulting in poor generalization and performance on new, unseen data.
12. **Underfitting**: A phenomenon where a ML model is too simple or lacks complexity, resulting in inadequate performance and difficulty in capturing patterns in the data.
13. **Cross-Validation**: A technique used to assess the performance and generalization of ML models by splitting the data into multiple subsets for training and evaluation.
14. **Ensemble Learning**: The technique of combining multiple ML models or predictions to improve overall performance, accuracy, or robustness.
15. **Hyperparameters**: Parameters that are set before training a ML model, such as learning rate, regularization strength, or number of hidden layers, which influence the model's behavior and performance.
16. **Decision Tree**: In a decision tree, the data is recursively split based on the values of the input features. 
    - It is a supervised machine learning algorithm that is widely used for both classification and regression tasks.
17. **Support Vector Machines:** A support vector machine (SVM) is a powerful supervised machine learning algorithm that can be used for both classification and regression tasks. 
    - SVM finds an optimal hyperplane in a high-dimensional feature space that separates the data points of different classes with the maximum margin.
18. **Data Preprocessing**: 
    - Data cleaning
    - Data integration
    - Data transformation [normalizing data]
    - Data reduction
    - Data discretization: Converting continuous date into categorical data by dividing it into discrete intervals.
    - Handeling missing data
    - Outlier detection
    - Data encoding
    - Feature scaling
19. **Feature Engineering**: Feature engineering is the proccess of selecting and transforming raw data into set of meaningful and relevant features that can be used for machine learning or data analysis tasks.
20. **Data splitting:** Splitting the dataset into 3 subsets,
    - Training set
    - Testing set
    - Validation set
21. **Cross Validation**: (k-fold) splitting dataset into k folds then train the model k times holding out a different fold each time for evaluation.
22. **Confusion matrix**: It shows the number of true positives, true negatives, false positives, and false negatives for each class in the classification problem. It shows the number of true positives, true negatives, false positives, and false negatives for each class in the classification problem.
    - Precision: measure the accuracy of positive predictions. p = TP/(TP+FP)
    - Recall: measure the completeness of positive prediction. p = TP/(TP+FN)
    - F score: harmonic mean of precision and recall. F1 = 2/(1/precision+1/ recall)
23. **Ensemble learning**: Ensemble learning is a machine learning technique that involves combining multiple individual models.
    - 2 types: Averaging and Boosting methods
24. **Voting classifier**:  A voting classifier is an ensemble learning method that combines multiple models to make predictions based on a majority vote.
25. **Bagging**: It is an ensemble learning technique in ML which trains multiple models independently on different subsets of the training data and then combining their predictions to make a final prediction.
    - Sampling is performed with replacement which means allowing the same data point to apear in multiple subsets.
26. **Pasting**: when sampling is performed without replacement.
27. **Boosting**: refers to any ensemble methods that can combine several weak learners into a strong learner.
28. **Stacking classifier**: A stacking classifier is an ensemble learning method that combines multiple models by training a meta-model to make predictions based on the outputs of the individual models.

<br><br>

**Deep Learning (DL):**

1. **Artificial Neural Networks (ANNs)**: ANNs are computing systems inspired by the structure and function of the human brain.
2. **Feedforward Neural Network:** Where the data flows in one direction from input to output.
3. **Reccurent Neural Network**: Where the data can flow in cycles and the network can have a ‘memory’ of past inputs.
4.
